# Spring AI Ollama Configuration
spring.ai.ollama.base-url=http://localhost:11434

# Specify the model to use (e.g., llama3, mistral, etc.)
spring.ai.ollama.chat.options.model=llama3